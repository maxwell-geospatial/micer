library(torch)

seed <- 42  # Replace with your desired seed
set.seed(seed)
torch_manual_seed(seed)
if (cuda_is_available()) {
  torch_cuda_manual_seed(seed)
  torch_cuda_manual_seed_all(seed)  # For multi-GPU setups
}
torch_backends_cudnn_deterministic(TRUE)
torch_backends_cudnn_benchmark(FALSE)


#Use transpose convolution to upsample tensors in the decoder
upConvBlk <- torch::nn_module(
  classname = "upConv",
  initialize = function(inChn,
                        outChn,
                        actFunc="relu",
                        negative_slope=0.01){
    
    self$upConv <- torch::nn_sequential(
      torch::nn_conv_transpose2d(inChn,
                                 outChn,
                                 kernel_size=c(2,2),
                                 stride=2),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      }
    )
  },
  
  forward = function(x){
    xx <- self$upConv(x)
    return(xx)
  }
)


#Used to created feature maps in the encoder and decoder
doubleConvBlk <- torch::nn_module(
  classname = "doubleConv",
  initialize = function(inChn,
                        outChn,
                        actFunc="relu",
                        negative_slope=0.01){
    
    self$dConv <- torch::nn_sequential(
      torch::nn_conv2d(inChn,
                       outChn,
                       kernel_size=c(3,3),
                       stride=1,
                       padding=1),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      },
      torch::nn_conv2d(outChn,
                       outChn,
                       kernel_size=c(3,3),
                       stride=1,
                       padding=1),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      }
    )
  },
  
  forward = function(x){
    xx <- self$dConv(x)
    return(xx)
  }
)

#Used to created feature maps in the encoder and decoder
#Includes residual connection
doubleConvBlkR <- torch::nn_module(
  classname = "doubleConvR",
  initialize = function(inChn,
                        outChn,
                        actFunc="relu",
                        negative_slope=0.01){
    
    self$dConv <- torch::nn_sequential(
      torch::nn_conv2d(inChn,
                       outChn,
                       kernel_size=c(3,3),
                       stride=1,
                       padding=1),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      },
      torch::nn_conv2d(outChn,
                       outChn,
                       kernel_size=c(3,3),
                       stride=1,
                       padding=1),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      }
    )
    
    self$skipPath <- featReduce(inChn=inChn,
                                outChn=outChn,
                                actFunc=actFunc,
                                negative_slope = negative_slope)
  },
  
  forward = function(x){
    xx <- self$dConv(x)
    xSP <- self$skipPath(x)
    return(xx + xSP)
  }
)

upSamp <- torch::nn_module(
  classname = "bilinearUpsample",
  initialize = function(scale_factor,
                        mode = "bilinear",
                        align_corners = FALSE) {
    self$scale_factor = scale_factor
    self$mode = mode
    self$align_corners = align_corners
  },
  
  forward = function(x) {
    torch::nnf_interpolate(x,
                           scale_factor = self$scale_factor,
                           mode = self$mode,
                           align_corners = self$align_corners)
  }
)


#Use 1x1 2D convolution to change the number of feature maps
featReduce <- torch::nn_module(
  initialize = function(inChn,
                        outChn,
                        actFunc="relu",
                        negative_slope=0.01){
    self$conv1_1 <- torch::nn_sequential(
      torch::nn_conv2d(inChn,
                       outChn,
                       kernel_size=c(1,1),
                       stride=1,
                       padding=0),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "lrelu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      }
    )
  },
  
  forward = function(x){
    xx <- self$conv1_1(x)
    return(xx)
  }
)

#Classification head
classifierBlk <- torch::nn_module(
  classname = "classificationHead",
  initialize = function(inChn, nCls){
    self$classifier <- torch::nn_conv2d(inChn,
                                        nCls,
                                        kernel_size=c(1,1),
                                        stride=1,
                                        padding=0)
  },
  
  forward = function(x){
    xx <- self$classifier(x)
    
    return(xx)
  }
)

#Define bottleneck component
bottleneck <- torch::nn_module(
  classname = "bottleneck",
  initialize = function(inChn,
                        outChn = 256,
                        actFunc = "relu",
                        negative_slope = 0.01){
    
    self$btnk <- doubleConvBlk(inChn=inChn,
                               outChn=outChn,
                               actFunc=actFunc,
                               negative_slope=negative_slope)
  },
  
  forward = function(x){
    xb <- self$btnk(x)
    return(xb)
  }
)

#Atrous Spatial Pyramid Pooling (ASPP) for use in bottleneck
asppComp <- torch::nn_module(
  classname = "asppComp",
  
  initialize = function(inChn,
                        outChn,
                        kernel_size,
                        stride,
                        padding,
                        dilation,
                        actFunc="relu",
                        negative_slope=negative_slope){
    
    self$aspp <-  torch::nn_sequential(
      torch::nn_conv2d(inChn,
                       outChn,
                       kernel_size,
                       stride,
                       padding,
                       dilation),
      torch::nn_batch_norm2d(outChn),
      if(actFunc == "l
         relu"){
        torch::nn_leaky_relu(inplace=TRUE,
                             negative_slope=negative_slope)
      }else if(actFunc == "swish"){
        torch::nn_silu(inplace=TRUE)
      }else{
        torch::nn_relu(inplace=TRUE)
      }
    )
  },
  
  forward = function(x){
    xx <- self$aspp(x)
    return(xx)
  }
)

#Combine ASPP components to create ASPP module
asppBlk <- torch::nn_module(
  classname = "aspp",
  initialize = function(inChn,
                        outChn,
                        dilChn=c(16,16,16,16,16),
                        dilRates=c(1,2,4,8,16),
                        actFunc="relu",
                        negative_slope=0.01){
    
    self$a1 <-asppComp(inChn=inChn,
                       outChn=dilChn[1],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[1],
                       dilation = dilRates[1],
                       actFunc=actFunc,
                       negative_slope=negative_slope)
    
    self$a2 <-asppComp(inChn=inChn,
                       outChn=dilChn[2],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[2],
                       dilation = dilRates[2],
                       actFunc=actFunc,
                       negative_slope=negative_slope)
    
    self$a3 <-asppComp(inChn=inChn,
                       outChn=dilChn[3],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[3],
                       dilation = dilRates[3],
                       actFunc=actFunc,
                       negative_slope=negative_slope)
    
    self$a4 <-asppComp(inChn=inChn,
                       outChn=dilChn[4],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[4],
                       dilation = dilRates[4],
                       actFunc=actFunc,
                       negative_slope=negative_slope)
    
    self$a5 <-asppComp(inChn=inChn,
                       outChn=dilChn[5],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[5],
                       dilation = dilRates[5],
                       actFunc=actFunc,
                       negative_slope=negative_slope)
    
    self$conv1_1 <- featReduce(dilChn[1]+dilChn[2]+dilChn[3]+dilChn[4]+dilChn[5],
                               outChn=outChn,
                               actFunc=actFunc,
                               negative_slope=negative_slope)
    
  },
  
  forward = function(x){
    x1 <- self$a1(x)
    x2 <- self$a2(x)
    x3 <- self$a3(x)
    x4 <- self$a4(x)
    x5 <- self$a5(x)
    xx <- torch::torch_cat(list(x1,x2,x3,x4,x5), dim=2)
    xx <- self$conv1_1(xx)
    
    return(xx)
    
  }
)




#Combine ASPP components to create ASPP module
#includes residual connection
asppBlkR <- torch::nn_module(
  classname = "asppR",
  initialize = function(inChn,
                        outChn,
                        dilChn=c(16,16,16,16,16),
                        dilRates=c(1,2,4,8,16),
                        actFunc="relu",
                        negative_slope=0.01){
    
    self$a1 <-asppComp(inChn=inChn,
                       outChn=dilChn[1],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[1],
                       dilation = dilRates[1],
                       actFunc="relu",
                       negative_slope=negative_slope)
    
    self$a2 <-asppComp(inChn=inChn,
                       outChn=dilChn[2],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[2],
                       dilation = dilRates[2],
                       actFunc="relu",
                       negative_slope=negative_slope)
    
    self$a3 <-asppComp(inChn=inChn,
                       outChn=dilChn[3],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[3],
                       dilation = dilRates[3],
                       actFunc="relu",
                       negative_slope=negative_slope)
    
    self$a4 <-asppComp(inChn=inChn,
                       outChn=dilChn[4],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[4],
                       dilation = dilRates[4],
                       actFunc="relu",
                       negative_slope=negative_slope)
    
    self$a5 <-asppComp(inChn=inChn,
                       outChn=dilChn[5],
                       kernel_size=c(3,3),
                       stride=1,
                       padding = dilRates[5],
                       dilation = dilRates[5],
                       actFunc="relu",
                       negative_slope=negative_slope)
    
    self$conv1_1 <- featReduce(dilChn[1]+dilChn[2]+dilChn[3]+dilChn[4]+dilChn[5],
                               outChn=outChn,
                               actFunc=actFunc,
                               negative_slope=negative_slope)
    
    self$skipPath <- featReduce(inChn=inChn,
                                outChn=outChn,
                                actFunc=actFunc,
                                negative_slope = negative_slope)
    
  },
  
  forward = function(x){
    x1 <- self$a1(x)
    x2 <- self$a2(x)
    x3 <- self$a3(x)
    x4 <- self$a4(x)
    x5 <- self$a5(x)
    xx <- torch::torch_cat(list(x1,x2,x3,x4,x5), dim=2)
    xx <- self$conv1_1(xx)
    
    xSC <- self$skipPath(x)
    
    return(xx+xSC)
    
  }
)


gaussConv <- nn_module(
  classname = "gaussConv",
  
  # Define the constructor
  initialize = function(inChn) {
    self$inChn <- inChn
    
    # Define the custom kernel as a non-trainable tensor
    gauss <- torch_tensor(c(1, 4, 6, 4, 1,
                            4, 16, 24, 16, 4,
                            6, 24, 36, 24, 6,
                            4, 16, 24, 16, 4,
                            1, 4, 6, 4, 1), device="cuda")$view(c(5, 5))$float() / 256
    gaussKernel <- torch_stack(lapply(1:inChn, function(i) {
      torch_stack(lapply(1:inChn, function(j) gauss), dim = 1)
    }), dim = 1)
    
    # Register the custom kernel as a buffer so it won't be updated during training
    self$custom_kernel <- gaussKernel
    self$custom_kernel$requires_grad_(FALSE)
  },
  
  # Define the forward pass
  forward = function(x) {
    # Use the registered buffer (non-trainable kernel) in the forward pass
    x <- nnf_conv2d(x, self$custom_kernel, stride = 1, padding = 2)
    return(x)
  }
)


dwsConv <- nn_module(
  classname = "dwsConv",
  
  # Define the constructor
  initialize = function(inFMs, outFMs, kPerFM = 1) {
    self$inFMs <- inFMs             # Number of input feature maps
    self$outFMs <- outFMs           # Number of output feature maps
    self$kPerFM <- kPerFM           # Number of feature maps per input feature map in depthwise convolution
    
    # Define the depthwise convolution layer
    self$dpthConv <- nn_conv2d(
      in_channels = inFMs,
      out_channels = inFMs * kPerFM,
      kernel_size = 3,
      stride = 1,
      padding = 1,
      groups = inFMs
    )
    
    # Define the pointwise convolution layer
    self$pntConv <- nn_conv2d(
      in_channels = inFMs * kPerFM,
      out_channels = outFMs,
      kernel_size = 1,
      stride = 1,
      padding = 0
    )
  },
  
  # Define the forward pass
  forward = function(x) {
    # Apply depthwise convolution
    x <- self$dpthConv(x)
    
    # Apply pointwise convolution
    x <- self$pntConv(x)
    
    return(x)
  }
)

terrainUNet <- nn_module(
  classname = "terrainUNet",
  
  # Define the constructor
  initialize = function(inChn=3, 
                        nCls=3, 
                        actFunc="lrelu", 
                        enChn = c(16,32,64,128),
                        dcChn = c(128,64,32,16),
                        btnChn = 256,
                        negative_slope = 0.01){
    self$inChn <- inChn
    self$nCls <- nCls
    self$actFunc <- "lrelu"
    self$enChn<- enChn
    self$dcChn <- dcChn
    self$btnChn <- btnChn
    self$negative_slope <- negative_slope
    
    self$gaussPyramid <- gaussConv(inChn)
    
    self$avgP <- nn_avg_pool2d(kernel_size = 2, 
                               stride = 2, 
                               padding = 0)
    self$maxP <- nn_max_pool2d(kernel_size = 2, 
                               stride = 2, 
                               padding = 0)
    
    self$encoder1 <- doubleConvBlk(inChn, 
                                    enChn[1],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$encoder2GP <- doubleConvBlk(inChn, 
                                      enChn[1],
                                      actFunc=actFunc,
                                      negative_slope=negative_slope)
    
    self$encoder2DWS <- dwsConv(inChn, 
                                enChn[1])
    
    self$encoder2 <- doubleConvBlk(enChn[1] * 2, 
                                    enChn[2],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$encoder3GP <- doubleConvBlk(inChn, 
                                      enChn[2],
                                      actFunc=actFunc,
                                      negative_slope=negative_slope)
    
    self$encoder3DWS <- dwsConv(inChn, 
                                enChn[2])
    
    self$encoder3 <- doubleConvBlk(enChn[2] * 2, 
                                    enChn[3],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$encoder4GP <- doubleConvBlk(inChn, 
                                      enChn[3],
                                      actFunc=actFunc,
                                      negative_slope=negative_slope)
    
    self$encoder4DWS <- dwsConv(inChn, 
                                enChn[3])
    
    self$encoder4 <- doubleConvBlk(enChn[3] * 2, 
                                    enChn[4],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$bottleneckGP <- doubleConvBlk(inChn, 
                                        enChn[4],
                                        actFunc=actFunc,
                                        negative_slope=negative_slope)
    
    self$bottleneckDWS <- dwsConv(inChn, 
                                  enChn[4])
    
    self$bottleneck <- doubleConvBlk(enChn[4] * 2, 
                                btnChn, 
                                actFunc=actFunc,
                                negative_slope=negative_slope)
    
    self$decoder1up <- upConvBlk(btnChn + enChn[4], 
                                 btnChn)
    
    self$decoder1 <- doubleConvBlk(enChn[4] + enChn[3] + btnChn, 
                                    dcChn[1],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$decoder2up <- upConvBlk(dcChn[1], 
                                 dcChn[1])
    
    self$decoder2 <- doubleConvBlk(enChn[3] + enChn[2] + dcChn[1], 
                                    dcChn[2],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$decoder3up <- upConvBlk(dcChn[2], 
                                 dcChn[2])
    
    self$decoder3 <- doubleConvBlk(enChn[2] + enChn[1] + dcChn[2], 
                                    dcChn[3],
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$decoder4up <- upConvBlk(dcChn[3], 
                                 dcChn[3])
    
    self$decoder4 <- doubleConvBlk(enChn[1] + inChn + dcChn[3], 
                                    dcChn[4], 
                                    actFunc=actFunc,
                                    negative_slope=negative_slope)
    
    self$classifier <- classifierBlk(dcChn[4], 
                                     nCls=nCls)
  },
  
  # Define the forward pass
  forward = function(x) {
    # Gaussian Pyramids and average pooling
    xGP1 <- self$gaussPyramid(x)
    xGP1a <- self$avgP(xGP1)
    xGP2 <- self$gaussPyramid(xGP1a)
    xGP2a <- self$avgP(xGP2)
    xGP3 <- self$gaussPyramid(xGP2a)
    xGP3a <- self$avgP(xGP3)
    xGP4 <- self$gaussPyramid(xGP3a)
    xGP4a <- self$avgP(xGP4)
    
    # Apply DWS conv to average pooled Gaussian pyramids
    xGPDWS1 <- self$encoder2DWS(xGP1a)
    xGPDWS2 <- self$encoder3DWS(xGP2a)
    xGPDWS3 <- self$encoder4DWS(xGP3a)
    xGPDWS4 <- self$bottleneckDWS(xGP4a)
    
    # Encoder gaussian pyramids
    e1GP <- self$encoder2GP(xGP1)
    e2GP <- self$encoder3GP(xGP2)
    e3GP <- self$encoder4GP(xGP3)
    e4GP <- self$bottleneckGP(xGP4)
    
    # Max pool gaussian pyramid encoder results
    e1GPa <- self$maxP(e1GP)
    e2GPa <- self$maxP(e2GP)
    e3GPa <- self$maxP(e3GP)
    e4GPa <- self$maxP(e4GP)
    
    # Encoder main path
    e1 <- self$encoder1(x)
    e1p <- self$maxP(e1)
    e2In <- torch_cat(list(e1p, xGPDWS1), dim = 2)
    e2 <- self$encoder2(e2In)
    e2p <- self$maxP(e2)
    e3In <- torch_cat(list(e2p, xGPDWS2), dim = 2)
    e3 <- self$encoder3(e3In)
    e3p <- self$maxP(e3)
    e4In <- torch_cat(list(e3p, xGPDWS3), dim = 2)
    e4 <- self$encoder4(e4In)
    e4p <- self$maxP(e4)
    
    # Bottleneck
    bIn <- torch_cat(list(e4p, xGPDWS4), dim = 2)
    bOut <- self$bottleneck(bIn)
    
    # Decoder
    bOutC <- torch_cat(list(bOut, e4GPa), dim = 2)
    
    d1upOut <- self$decoder1up(bOutC)
    d1In <- torch_cat(list(d1upOut, e4, e3GPa), dim = 2)
    d1Out <- self$decoder1(d1In)
    
    d2upOut <- self$decoder2up(d1Out)
    d2In <- torch_cat(list(d2upOut, e3, e2GPa), dim = 2)
    d2Out <- self$decoder2(d2In)
    
    d3upOut <- self$decoder3up(d2Out)
    d3In <- torch_cat(list(d3upOut, e2, e1GPa), dim = 2)
    d3Out <- self$decoder3(d3In)
    
    d4upOut <- self$decoder4up(d3Out)
    d4In <- torch_cat(list(d4upOut, e1, x), dim = 2)
    d4Out <- self$decoder4(d4In)
    
    # Classifier head
    x <- self$classifier(d4Out)
    
    return(x)
  }
)





multiScaleInputUNet <- nn_module(
  classname = "multiScaleInputUNet",
  
  # Define the constructor
  initialize = function(inChn=3, 
                        nCls=3, 
                        actFunc="lrelu", 
                        enChn = c(16,32,64,128),
                        dcChn = c(128,64,32,16),
                        btnChn = 256,
                        negative_slope = 0.01){
    self$inChn <- inChn
    self$nCls <- nCls
    self$actFunc <- "lrelu"
    self$enChn<- enChn
    self$dcChn <- dcChn
    self$btnChn <- btnChn
    self$negative_slope <- negative_slope
    
    self$gaussPyramid <- gaussConv(inChn)
    
    self$avgP <- nn_avg_pool2d(kernel_size = 2, 
                               stride = 2, 
                               padding = 0)
    self$maxP <- nn_max_pool2d(kernel_size = 2, 
                               stride = 2, 
                               padding = 0)
    
    self$encoder1 <- doubleConvBlk(inChn, 
                                   enChn[1],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$encoder2 <- doubleConvBlk(enChn[1] + inChn, 
                                   enChn[2],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$encoder3 <- doubleConvBlk(enChn[2] + inChn, 
                                   enChn[3],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    
    self$encoder4 <- doubleConvBlk(enChn[3] + inChn, 
                                   enChn[4],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$bottleneck <- doubleConvBlk(enChn[4] + inChn, 
                                     btnChn, 
                                     actFunc=actFunc,
                                     negative_slope=negative_slope)
    
    self$decoder1up <- upConvBlk(btnChn, 
                                 btnChn)
    
    self$decoder1 <- doubleConvBlk(enChn[4] + btnChn, 
                                   dcChn[1],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$decoder2up <- upConvBlk(dcChn[1], 
                                 dcChn[1])
    
    self$decoder2 <- doubleConvBlk(enChn[3] + dcChn[1], 
                                   dcChn[2],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$decoder3up <- upConvBlk(dcChn[2], 
                                 dcChn[2])
    
    self$decoder3 <- doubleConvBlk(enChn[2] + dcChn[2], 
                                   dcChn[3],
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$decoder4up <- upConvBlk(dcChn[3], 
                                 dcChn[3])
    
    self$decoder4 <- doubleConvBlk(enChn[1] + dcChn[3], 
                                   dcChn[4], 
                                   actFunc=actFunc,
                                   negative_slope=negative_slope)
    
    self$classifier <- classifierBlk(dcChn[4], 
                                     nCls=nCls)
  },
  
  # Define the forward pass
  forward = function(x) {
    # Gaussian Pyramids and average pooling
    xGP1 <- self$gaussPyramid(x)
    xGP1a <- self$avgP(xGP1)
    xGP2 <- self$gaussPyramid(xGP1a)
    xGP2a <- self$avgP(xGP2)
    xGP3 <- self$gaussPyramid(xGP2a)
    xGP3a <- self$avgP(xGP3)
    xGP4 <- self$gaussPyramid(xGP3a)
    xGP4a <- self$avgP(xGP4)
    
    # Encoder main path
    e1 <- self$encoder1(x)
    e1p <- self$maxP(e1)
    e2In <- torch_cat(list(e1p, xGP1a), dim = 2)
    e2 <- self$encoder2(e2In)
    e2p <- self$maxP(e2)
    e3In <- torch_cat(list(e2p, xGP2a), dim = 2)
    e3 <- self$encoder3(e3In)
    e3p <- self$maxP(e3)
    e4In <- torch_cat(list(e3p, xGP3a), dim = 2)
    e4 <- self$encoder4(e4In)
    e4p <- self$maxP(e4)
    
    # Bottleneck
    bIn <- torch_cat(list(e4p, xGP4a), dim = 2)
    bOut <- self$bottleneck(bIn)
    
    # Decoder
    bOutC <- torch_cat(list(bOut, e4GPa), dim = 2)
    
    d1upOut <- self$decoder1up(bOutC)
    d1Out <- self$decoder1(d1upOut)
    
    d2upOut <- self$decoder2up(d1Out)
    d2Out <- self$decoder2(d2upOut)
    
    d3upOut <- self$decoder3up(d2Out)
    d3Out <- self$decoder3(d3upOut)
    
    d4upOut <- self$decoder4up(d3Out)
    d4Out <- self$decoder4(d4upOut)
    
    # Classifier head
    x <- self$classifier(d4Out)
    
    return(x)
  }
)