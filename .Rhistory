data(mcData)
ciResultsMC <- miceCI(rep=1000,
frac=.7,
mcData$ref,
mcData$pred,
lowPercentile=0.025,
highPercentile=0.975,
mappings=c("Barren",
"Forest",
"Impervious",
"Low Vegetation",
"Mixed Dev",
"Water"),
multiclass=TRUE)
print(ciResultsMC)
data(biData)
ciResultsBi <- miceCI(rep=1000,
frac=.7,
biData$ref,
biData$pred,
lowPercentile=0.025,
highPercentile=0.975,
mappings = c("Mined",
"Not Mined"),
multiclass=FALSE,
positiveIndex=1)
print(ciResultsBi)
data(compareData)
compareResult <- miceCompare(ref=compareData$ref,
result1=compareData$rfPred,
result2=compareData$dtPred,
reps=1000,
frac=.7)
print(compareResult)
data(compareData)
compareResult <- miceCompare(ref=compareData$ref,
result1=compareData$rfPred,
result2=compareData$dtPred,
reps=1000,
frac=.7)
print(compareResult)
data(compareData)
set.seed(42)
compareResult <- miceCompare(ref=compareData$ref,
result1=compareData$rfPred,
result2=compareData$dtPred,
reps=1000,
frac=.7)
print(compareResult)
data(compareData)
set.seed(42)
compareResult <- miceCompare(ref=compareData$ref,
result1=compareData$rfPred,
result2=compareData$dtPred,
reps=1000,
frac=.7)
print(compareResult)
devtools::use_gpl_license()
usethis::use_gpl3_license()
usethis::use_readme_rmd()
devtools::document()
devtools::check(cran=TRUE)
View(ciResultsBi)
devtools::check(cran=TRUE)
devtools::check(cran=TRUE)
devtools::check(cran=TRUE)
usethis::use_readme_md()
devtools::document()
library(micer)
devtools::build_manual()
devtools::build_site()
setwd("C:/Users/vidcg/Dropbox/code_dev/miceR/micer/")
devtools::build_site()
devtools::build_site()
devtools::build_site()
devtools::build_site()
library(micer)
devtools::build_site()
data(biData)
miceResultBI <- mice(biData$ref,
biData$pred,
mappings = c("Mined",
"Not Mined"),
multiclass=FALSE,
positiveIndex=1)
print(miceResultBI)
biData |> group_by(ref) |> count()
biData |> dplyr::group_by(ref) |> dplyr::count()
biData |> dplyr::group_by(pred) |> dplyr::count()
table(biData$ref, biData$pred)
table(biData$pred, biData$ref)
mcData |> dplyr::group_by(ref) |> dplyr::count()
mice <- function(reference, #Factor of correct/reference labels
prediction,#Factor of predicted labels
mappings=levels(as.factor(reference)), #Names of classes (if not provided, will use factor levels)
multiclass = TRUE, #TRUE for multiclass, FALSE for binary
positiveIndex = 1 #Index for the positive case (only used for binary classification
){
positiveIndex <- as.numeric(positiveIndex) #Make sure index is numeric
ctab <- table(prediction, reference) #generate contingency table
colnames(ctab) <- mappings #Apply class names to columns
rownames(ctab) <- mappings #Apply class names to rows
dimnames(ctab) <- setNames(dimnames(ctab),c("Predicted", "Reference")) #Label row axis as "predicted" and column axis as "reference"
refCnts <- colSums(ctab) #Get column total counts
names(refCnts) <- mappings #Name column counts
predCnts <- rowSums(ctab) #Get row total counts
names(predCnts) <- mappings #Name row counts
oa <- sum(diag(ctab))/sum(ctab) #Calcualte overall accuracy
sumCols <- colSums(ctab)
sumRows <- rowSums(ctab)
sumColsN <- (sumCols/sum(sumCols))+.00001 #calculate nj/n
sumColsN2 <- sumColsN*sumColsN #square nj/n
oa0 <- sum(sumColsN2) #calculate random model correction
mice <- (oa-oa0)/(1-oa0) #Calculate MICE
ua <- diag(ctab)/(sumRows +.00001) #Calculate all class user's accuracies (i.e., precisions)
pa <- diag(ctab)/(sumCols +.00001) #Calculate all class producer's accuracies (i.e., recalls)
f1 <- (2*ua*pa)/(ua+pa) #Calculate all class F1-scores ((2*Precision*Recall)/(precision+recall))
rtbice <- (pa - sumColsN)/(1- sumColsN) #Calculate all reference-total-based image classification efficacies
ctbice <- (ua - sumColsN)/(1- sumColsN) #Calcualte all classification-total-based image classification efficacies
f1Efficacy <- (2*rtbice*ctbice)/(rtbice+ctbice) #Calculate F1-scores with efficacy-based correction
if(multiclass==TRUE){
#Add names to all vectors
names(ua) <- mappings
names(pa) <- mappings
names(rtbice) <- mappings
names(ctbice) <- mappings
names(f1) <- mappings
names(f1Efficacy) <- mappings
#Macro-average all UAs/precisions, PAs/recalls, and F1-scores for class-aggregated metrics
macroUA <- mean(ua)
macroPA <- mean(pa)
macroF1 <- (2*macroUA*macroPA)/(macroUA+macroPA)
#Macro-average all efficiacy-based measusures for class-aggregated metrics
macroRTBICE <- mean(rtbice)
macroCTBICE <- mean(ctbice)
macrof1Efficacy <- (2*macroRTBICE*macroCTBICE)/(macroRTBICE+macroCTBICE)
#Return list object for multiclass classification
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
overallAccuracy = oa,
MICE = mice,
usersAccuracies = ua,
CTBICEs = ctbice,
producersAccuracies = pa,
RTBICEs = rtbice,
f1Scores = f1,
f1Efficacies = f1Efficacy,
macroPA = macroPA,
macroRTBUCE = macroRTBICE,
macroUA = macroUA,
macroCTBICE = macroCTBICE,
macroF1 = macroF1,
macroF1Efficacy = macrof1Efficacy
)
)
}else{
#Return list object for binary classification
negativeIndex = 2 - positiveIndex
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
positiveCase = mappings[positiveIndex],
overallAccuracy = oa,
mice = mice,
Precision = unname(ua)[positiveIndex],
precisionEfficacy = unname(ctbice)[positiveIndex],
NPV = unname(ua)[negativeIndex],
npvEfficacy = unname(ctbice)[negativeIndex],
Recall = unname(pa)[positiveIndex],
recallEfficacy = unname(rtbice)[positiveIndex],
Specificity = unname(pa)[negativeIndex],
specificityEfficicacy = unname(rtbice)[negativeIndex],
f1Score = unname(f1)[positiveIndex],
f1ScoreEfficacy = unname(f1Efficacy)[positiveIndex]
)
)
}
}
data(biData)
mice(biData$ref,
biData$pred,
mappings = c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)
yardstick::specificity_vec(truth=biData$ref, estimate=biData$pred)
yardstick::recall_vec(truth=biData$ref, estimate=biData$pred)
yardstick::precision_vec(truth=biData$ref, estimate=biData$pred)
yardstick::f_meas_vec(truth=biData$ref, estimate=biData$pred)
yardstick::npv_vec(truth=biData$ref, estimate=biData$pred)
miceCM <- function(cm,#Factor of predicted labels
mappings=levels(as.factor(row.names(cm))), #Names of classes (if not provided, will use factor levels)
multiclass = TRUE, #TRUE for multiclass, FALSE for binary
positiveIndex = 1 #Index for the positive case (only used for binary classification
){
positiveIndex <- as.numeric(positiveIndex) #Make sure index is numeric
ctab <- cm #generate contingency table
colnames(ctab) <- mappings #Apply class names to columns
rownames(ctab) <- mappings #Apply class names to rows
dimnames(ctab) <- setNames(dimnames(ctab),c("Predicted", "Reference")) #Label row axis as "predicted" and column axis as "reference"
refCnts <- colSums(ctab) #Get column total counts
names(refCnts) <- mappings #Name column counts
predCnts <- rowSums(ctab) #Get row total counts
names(predCnts) <- mappings #Name row counts
oa <- sum(diag(ctab))/sum(ctab) #Calcualte overall accuracy
sumCols <- colSums(ctab)
sumRows <- rowSums(ctab)
sumColsN <- (sumCols/sum(sumCols))+.00001 #calculate nj/n
sumColsN2 <- sumColsN*sumColsN #square nj/n
oa0 <- sum(sumColsN2) #calculate random model correction
mice <- (oa-oa0)/(1-oa0) #Calculate MICE
ua <- diag(ctab)/(sumRows +.00001) #Calculate all class user's accuracies (i.e., precisions)
pa <- diag(ctab)/(sumCols +.00001) #Calculate all class producer's accuracies (i.e., recalls)
f1 <- (2*ua*pa)/(ua+pa) #Calculate all class F1-scores ((2*Precision*Recall)/(precision+recall))
rtbice <- (pa - sumColsN)/(1- sumColsN) #Calculate all reference-total-based image classification efficacies
ctbice <- (ua - sumColsN)/(1- sumColsN) #Calcualte all classification-total-based image classification efficacies
f1Efficacy <- (2*rtbice*ctbice)/(rtbice+ctbice) #Calculate F1-scores with efficacy-based correction
if(multiclass==TRUE){
#Add names to all vectors
names(ua) <- mappings
names(pa) <- mappings
names(rtbice) <- mappings
names(ctbice) <- mappings
names(f1) <- mappings
names(f1Efficacy) <- mappings
#Macro-average all UAs/precisions, PAs/recalls, and F1-scores for class-aggregated metrics
macroUA <- mean(ua)
macroPA <- mean(pa)
macroF1 <- (2*macroUA*macroPA)/(macroUA+macroPA)
#Macro-average all efficiacy-based measusures for class-aggregated metrics
macroRTBICE <- mean(rtbice)
macroCTBICE <- mean(ctbice)
macrof1Efficacy <- (2*macroRTBICE*macroCTBICE)/(macroRTBICE+macroCTBICE)
#Return list object for multiclass classification
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
overallAccuracy = oa,
MICE = mice,
usersAccuracies = ua,
CTBICEs = ctbice,
producersAccuracies = pa,
RTBICEs = rtbice,
f1Scores = f1,
f1Efficacies = f1Efficacy,
macroPA = macroPA,
macroRTBUCE = macroRTBICE,
macroUA = macroUA,
macroCTBICE = macroCTBICE,
macroF1 = macroF1,
macroF1Efficacy = macrof1Efficacy
)
)
}else{
#Return list object for binary classification
negativeIndex = 3 - positiveIndex
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
positiveCase = mappings[positiveIndex],
overallAccuracy = oa,
mice = mice,
Precision = unname(ua)[positiveIndex],
precisionEfficacy = unname(ctbice)[positiveIndex],
NPV = unname(ua)[negativeIndex],
npvEfficacy = unname(ctbice)[negativeIndex],
Recall = unname(pa)[positiveIndex],
recallEfficacy = unname(rtbice)[positiveIndex],
Specificity = unname(pa)[negativeIndex],
specificityEfficicacy = unname(rtbice)[negativeIndex],
f1Score = unname(f1)[positiveIndex],
f1ScoreEfficacy = unname(f1Efficacy)[positiveIndex]
)
)
}
}
data(biData)
mice(biData$ref,
biData$pred,
mappings = c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)
yardstick::npv_vec(truth=biData$ref, estimate=biData$pred)
yardstick::f_meas_vec(truth=biData$ref, estimate=biData$pred)
yardstick::precision_vec(truth=biData$ref, estimate=biData$pred)
yardstick::recall_vec(truth=biData$ref, estimate=biData$pred)
yardstick::specificity_vec(truth=biData$ref, estimate=biData$pred)
mice <- function(reference, #Factor of correct/reference labels
prediction,#Factor of predicted labels
mappings=levels(as.factor(reference)), #Names of classes (if not provided, will use factor levels)
multiclass = TRUE, #TRUE for multiclass, FALSE for binary
positiveIndex = 1 #Index for the positive case (only used for binary classification
){
positiveIndex <- as.numeric(positiveIndex) #Make sure index is numeric
ctab <- table(prediction, reference) #generate contingency table
colnames(ctab) <- mappings #Apply class names to columns
rownames(ctab) <- mappings #Apply class names to rows
dimnames(ctab) <- setNames(dimnames(ctab),c("Predicted", "Reference")) #Label row axis as "predicted" and column axis as "reference"
refCnts <- colSums(ctab) #Get column total counts
names(refCnts) <- mappings #Name column counts
predCnts <- rowSums(ctab) #Get row total counts
names(predCnts) <- mappings #Name row counts
oa <- sum(diag(ctab))/sum(ctab) #Calcualte overall accuracy
sumCols <- colSums(ctab)
sumRows <- rowSums(ctab)
sumColsN <- (sumCols/sum(sumCols))+.00001 #calculate nj/n
sumColsN2 <- sumColsN*sumColsN #square nj/n
oa0 <- sum(sumColsN2) #calculate random model correction
mice <- (oa-oa0)/(1-oa0) #Calculate MICE
ua <- diag(ctab)/(sumRows +.00001) #Calculate all class user's accuracies (i.e., precisions)
pa <- diag(ctab)/(sumCols +.00001) #Calculate all class producer's accuracies (i.e., recalls)
f1 <- (2*ua*pa)/(ua+pa) #Calculate all class F1-scores ((2*Precision*Recall)/(precision+recall))
rtbice <- (pa - sumColsN)/(1- sumColsN) #Calculate all reference-total-based image classification efficacies
ctbice <- (ua - sumColsN)/(1- sumColsN) #Calculate all classification-total-based image classification efficacies
f1Efficacy <- (2*rtbice*ctbice)/(rtbice+ctbice) #Calculate F1-scores with efficacy-based correction
if(multiclass==TRUE){
#Add names to all vectors
names(ua) <- mappings
names(pa) <- mappings
names(rtbice) <- mappings
names(ctbice) <- mappings
names(f1) <- mappings
names(f1Efficacy) <- mappings
#Macro-average all UAs/precisions, PAs/recalls, and F1-scores for class-aggregated metrics
macroUA <- mean(ua)
macroPA <- mean(pa)
macroF1 <- (2*macroUA*macroPA)/(macroUA+macroPA)
#Macro-average all efficiacy-based measusures for class-aggregated metrics
macroRTBICE <- mean(rtbice)
macroCTBICE <- mean(ctbice)
macrof1Efficacy <- (2*macroRTBICE*macroCTBICE)/(macroRTBICE+macroCTBICE)
#Return list object for multiclass classification
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
overallAccuracy = oa,
MICE = mice,
usersAccuracies = ua,
CTBICEs = ctbice,
producersAccuracies = pa,
RTBICEs = rtbice,
f1Scores = f1,
f1Efficacies = f1Efficacy,
macroPA = macroPA,
macroRTBUCE = macroRTBICE,
macroUA = macroUA,
macroCTBICE = macroCTBICE,
macroF1 = macroF1,
macroF1Efficacy = macrof1Efficacy
)
)
}else{
#Return list object for binary classification
negativeIndex = 3 - positiveIndex
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
positiveCase = mappings[positiveIndex],
overallAccuracy = oa,
mice = mice,
Precision = unname(ua)[positiveIndex],
precisionEfficacy = unname(ctbice)[positiveIndex],
NPV = unname(ua)[negativeIndex],
npvEfficacy = unname(ctbice)[negativeIndex],
Recall = unname(pa)[positiveIndex],
recallEfficacy = unname(rtbice)[positiveIndex],
Specificity = unname(pa)[negativeIndex],
specificityEfficicacy = unname(rtbice)[negativeIndex],
f1Score = unname(f1)[positiveIndex],
f1ScoreEfficacy = unname(f1Efficacy)[positiveIndex]
)
)
}
}
data(biData)
mice(biData$ref,
biData$pred,
mappings = c("Mined", "Not Mined"),
multiclass=FALSE,
positiveIndex=1)
#Multiclass example
data(mcData)
cmMC <- table(mcData$ref, mcData$pred)
miceCM(cmMC,
mappings=c("Barren", "Forest", "Impervious", "Low Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)
yardstick::f_meas_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
mice <- function(reference, #Factor of correct/reference labels
prediction,#Factor of predicted labels
mappings=levels(as.factor(reference)), #Names of classes (if not provided, will use factor levels)
multiclass = TRUE, #TRUE for multiclass, FALSE for binary
positiveIndex = 1 #Index for the positive case (only used for binary classification
){
positiveIndex <- as.numeric(positiveIndex) #Make sure index is numeric
ctab <- table(prediction, reference) #generate contingency table
colnames(ctab) <- mappings #Apply class names to columns
rownames(ctab) <- mappings #Apply class names to rows
dimnames(ctab) <- setNames(dimnames(ctab),c("Predicted", "Reference")) #Label row axis as "predicted" and column axis as "reference"
refCnts <- colSums(ctab) #Get column total counts
names(refCnts) <- mappings #Name column counts
predCnts <- rowSums(ctab) #Get row total counts
names(predCnts) <- mappings #Name row counts
oa <- sum(diag(ctab))/sum(ctab) #Calcualte overall accuracy
sumCols <- colSums(ctab)
sumRows <- rowSums(ctab)
sumColsN <- (sumCols/sum(sumCols))+.00001 #calculate nj/n
sumColsN2 <- sumColsN*sumColsN #square nj/n
oa0 <- sum(sumColsN2) #calculate random model correction
mice <- (oa-oa0)/(1-oa0) #Calculate MICE
ua <- diag(ctab)/(sumRows +.00001) #Calculate all class user's accuracies (i.e., precisions)
pa <- diag(ctab)/(sumCols +.00001) #Calculate all class producer's accuracies (i.e., recalls)
f1 <- (2*ua*pa)/(ua+pa) #Calculate all class F1-scores ((2*Precision*Recall)/(precision+recall))
rtbice <- (pa - sumColsN)/(1- sumColsN) #Calculate all reference-total-based image classification efficacies
ctbice <- (ua - sumColsN)/(1- sumColsN) #Calculate all classification-total-based image classification efficacies
f1Efficacy <- (2*rtbice*ctbice)/(rtbice+ctbice) #Calculate F1-scores with efficacy-based correction
if(multiclass==TRUE){
#Add names to all vectors
names(ua) <- mappings
names(pa) <- mappings
names(rtbice) <- mappings
names(ctbice) <- mappings
names(f1) <- mappings
names(f1Efficacy) <- mappings
#Macro-average all UAs/precisions, PAs/recalls, and F1-scores for class-aggregated metrics
macroUA <- mean(ua)
macroPA <- mean(pa)
macroF1 <- (2*macroUA*macroPA)/(macroUA+macroPA)
#Macro-average all efficiacy-based measusures for class-aggregated metrics
macroRTBICE <- mean(rtbice)
macroCTBICE <- mean(ctbice)
macrof1Efficacy <- (2*macroRTBICE*macroCTBICE)/(macroRTBICE+macroCTBICE)
#Return list object for multiclass classification
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
overallAccuracy = oa,
MICE = mice,
usersAccuracies = ua,
CTBICEs = ctbice,
producersAccuracies = pa,
RTBICEs = rtbice,
f1Scores = f1,
f1Efficacies = f1Efficacy,
macroPA = macroPA,
macroRTBUCE = macroRTBICE,
macroUA = macroUA,
macroCTBICE = macroCTBICE,
macroF1 = macroF1,
macroF1Efficacy = macrof1Efficacy
)
)
}else{
#Return list object for binary classification
negativeIndex = 3 - positiveIndex
return(list(Mappings = mappings,
confusionMatrix = ctab,
referenceCounts = refCnts,
predictionCounts = predCnts,
positiveCase = mappings[positiveIndex],
overallAccuracy = oa,
mice = mice,
Precision = unname(ua)[positiveIndex],
precisionEfficacy = unname(ctbice)[positiveIndex],
NPV = unname(ua)[negativeIndex],
npvEfficacy = unname(ctbice)[negativeIndex],
Recall = unname(pa)[positiveIndex],
recallEfficacy = unname(rtbice)[positiveIndex],
Specificity = unname(pa)[negativeIndex],
specificityEfficicacy = unname(rtbice)[negativeIndex],
f1Score = unname(f1)[positiveIndex],
f1ScoreEfficacy = unname(f1Efficacy)[positiveIndex]
)
)
}
}
#Multiclass example
data(mcData)
cmMC <- table(mcData$ref, mcData$pred)
miceCM(cmMC,
mappings=c("Barren", "Forest", "Impervious", "Low Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)
yardstick::f_meas_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
#' #Multiclass example
data(mcData)
#mice(mcData$ref,
mcData$pred,
#' #Multiclass example
data(mcData)
mice(mcData$ref,
mcData$pred,
mappings=c("Barren", "Forest", "Impervious", "Low
Vegetation", "Mixed Dev", "Water"),
multiclass=TRUE)
yardstick::f_meas_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
data(mcData)
yardstick::npv_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
yardstick::f_meas_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
yardstick::precision_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
yardstick::recall_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
yardstick::specificity_vec(truth=mcData$ref, estimate=mcData$pred, estimator="macro")
library(micer)
devtools::document()
setwd("C:/Users/vidcg/Dropbox/code_dev/miceR/micer/")
devtools::document()
devtools::check(cran=TRUE)
